{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import torch \n",
    "\n",
    "import resemblyzer\n",
    "\n",
    "from melgan.model.generator import Generator\n",
    "\n",
    "from hparams import create_hparams\n",
    "from model import Parrot\n",
    "from reader.symbols import ph2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparam_string = ''\n",
    "hparams = create_hparams(hparam_string)\n",
    "checkpoint_path = '../runs/outdir_embeds_by_resemblyzer_23jul2020/checkpoint_51000'\n",
    "\n",
    "# init melgan\n",
    "ckpt = torch.load('../runs/melgan_TEMP/librispeech_41cec78_0525.pt')\n",
    "melgan_vocoder = Generator(80).cuda()\n",
    "melgan_vocoder.load_state_dict(ckpt['model_g'])\n",
    "melgan_vocoder.eval()\n",
    "\n",
    "# init parrot (nonparaseq2seq2020)\n",
    "parrot_model = Parrot(hparams).cuda()\n",
    "parrot_model.load_state_dict(torch.load(checkpoint_path)[\"state_dict\"])\n",
    "\n",
    "# init mean statistics for mel normalization\n",
    "mel_mean, mel_std = np.load(hparams.mel_mean_std)\n",
    "\n",
    "# init speaker embeddict\n",
    "speaker_embedder = resemblyzer.VoiceEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count params in parrot model\n",
    "print(np.sum([torch.numel(param) for param in parrot_model.parameters()]))\n",
    "print(np.sum([torch.numel(param) for param in parrot_model.text_encoder.parameters()]))\n",
    "print(np.sum([torch.numel(param) for param in parrot_model.audio_seq2seq.parameters()]))\n",
    "# print(np.sum([torch.numel(param) for param in parrot_model.speaker_encoder.parameters()]))\n",
    "print(np.sum([torch.numel(param) for param in parrot_model.speaker_classifier.parameters()]))\n",
    "print(np.sum([torch.numel(param) for param in parrot_model.decoder.parameters()]))\n",
    "print(np.sum([torch.numel(param) for param in parrot_model.postnet.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parrot helper functions\n",
    "# audio_in1,  _ = librosa.load('../runs/outdir_21jul2020/p246_018.flac', sr=16000)\n",
    "# audio_in2,  _ = librosa.load('../runs/outdir_21jul2020/V3-6617-314-nl_sp.flac', sr=16000)\n",
    "# audio_ref, _ = librosa.load('../runs/outdir_21jul2020/p257_012.flac', sr=16000)\n",
    "\n",
    "# audio_ref, _ = librosa.load('../runs/outdir_21jul2020/V3-6617-314-nl_sp.flac', sr=16000)\n",
    "# audio_ref, _ = librosa.load('../runs/outdir_21jul2020/s107u011n.flac', sr=16000)\n",
    "\n",
    "# path_ref = '../runs/outdir_21jul2020/s107u011n.flac'\n",
    "# path_ref = '../runs/outdir_21jul2020/V3-6617-314-nl_sp.flac'\n",
    "# path_ref = '../runs/outdir_21jul2020/p257_012.flac'\n",
    "# path_inp = '../runs/outdir_21jul2020/p246_018.flac'\n",
    "path_inp = '../runs/outdir_21jul2020/p257_012.flac'\n",
    "\n",
    "audio_ref, _ = librosa.load(path_ref, sr=16000)\n",
    "audio_in,  _ = librosa.load(path_inp, sr=16000)\n",
    "\n",
    "def audio_to_spect_for_parrot(audio):\n",
    "    spec = np.abs(librosa.stft(y=audio, n_fft=2048, hop_length=256, win_length=1024, window='hann', center=True, pad_mode='reflect'))\n",
    "    melspect =librosa.feature.melspectrogram(S=spec, sr=16000, n_mels=80, htk=False)\n",
    "    logmelspect = np.log(np.clip(melspect, a_min=1e-5, a_max=None)).astype(np.float32)\n",
    "    normlogmelspect = (logmelspect - mel_mean[:, None]) / mel_std[:, None]\n",
    "    normlogmelspect = torch.cuda.FloatTensor(normlogmelspect)[None, :]\n",
    "    return normlogmelspect\n",
    "\n",
    "def mel_to_wav(mel_input):\n",
    "    mean = torch.FloatTensor(mel_mean)[:,None].cuda()\n",
    "    std = torch.FloatTensor(mel_std)[:,None].cuda()\n",
    "    mel_input = 1.2 * mel_input * std + mean\n",
    "    mel_input = torch.log(torch.clamp(torch.exp(mel_input), 1e-5)) # TODO: clamp in logspace?\n",
    "\n",
    "    audio = melgan_vocoder.inference(mel_input).float() / 32768.0\n",
    "\n",
    "    return audio.data.cpu().numpy()\n",
    "\n",
    "def text_to_phoneme_to_idc(text):\n",
    "    # ONLY WORKS FOR US-EN\n",
    "    from phonemizer.phonemize import phonemize\n",
    "    from phonemizer.backend import FestivalBackend\n",
    "    from phonemizer.separator import Separator\n",
    "\n",
    "    phones = phonemize(\n",
    "        text,\n",
    "        language  = 'en-us',\n",
    "        backend   = 'festival',\n",
    "        separator = Separator(\n",
    "            phone    = ' ',\n",
    "            syllable = '',\n",
    "            word     = ''\n",
    "        )\n",
    "    )\n",
    "        \n",
    "    idc = torch.cuda.LongTensor([ph2id[ph] for ph in phones.split()])[None, :]\n",
    "    return idc\n",
    "\n",
    "mel_in = audio_to_spect_for_parrot(audio_in)\n",
    "mel_ref = torch.zeros_like(mel_in) # DUMMY INPUT\n",
    "spkr_embed = speaker_embedder.embed_utterance(resemblyzer.preprocess_wav(path_ref))\n",
    "spkr_embed = torch.cuda.FloatTensor(spkr_embed)[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VC\n",
    "\n",
    "text_input_padded = torch.cuda.LongTensor([[0, 0, 0]]) # dummy input\n",
    "mel_padded = mel_in # as it is a single item, no need to pad\n",
    "text_lengths = None\n",
    "mel_lengths = mel_padded.size(-1)\n",
    "\n",
    "# inp_list = [text_input_padded, mel_padded, text_lengths, mel_lengths]\n",
    "inp_list = [text_input_padded, mel_padded, text_lengths, mel_lengths, spkr_embed]\n",
    "\n",
    "y_pred = parrot_model.inference(inp_list, False, mel_ref, hparams.beam_width)\n",
    "\n",
    "mel_output = y_pred[1]\n",
    "\n",
    "audio_out = mel_to_wav(mel_output)\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "ipd.display(ipd.Audio(audio_in,  rate=16000))\n",
    "ipd.display(ipd.Audio(audio_ref, rate=16000))\n",
    "ipd.display(ipd.Audio(audio_out, rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTS\n",
    "\n",
    "text = \"Puppies are cute. Cats are fluffy\"\n",
    "\n",
    "text_input_padded = text_to_phoneme_to_idc(text)\n",
    "mel_padded = torch.zeros_like(mel_padded) # dummy input \n",
    "text_lengths = [text_input_padded.size(-1)]\n",
    "mel_lengths = mel_padded.size(-1)\n",
    "\n",
    "# inp_list = [text_input_padded, mel_padded, text_lengths, mel_lengths]\n",
    "inp_list = [text_input_padded, mel_padded, text_lengths, mel_lengths, spkr_embed]\n",
    "\n",
    "y_pred = parrot_model.inference(inp_list, True, mel_ref, hparams.beam_width)\n",
    "\n",
    "mel_output = y_pred[1]\n",
    "\n",
    "audio_out = mel_to_wav(mel_output)\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "ipd.display(ipd.Audio(audio_ref, rate=16000))\n",
    "ipd.display(ipd.Audio(audio_out, rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
